{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 128, 647, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.pad_1 (TFOpLambda) (None, 130, 649, 1)  0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 128, 647, 32) 320         tf.compat.v1.pad_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 128, 647, 32) 128         conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "incept_1_input (MaxPooling2D)   (None, 128, 161, 32) 0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 128, 161, 32) 128         incept_1_input[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 128, 161, 32) 128         incept_1_input[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 128, 161, 64) 2112        batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 128, 161, 16) 528         batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)  (None, 128, 161, 32) 0           incept_1_input[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 128, 161, 32) 128         incept_1_input[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 128, 161, 64) 256         conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 128, 161, 16) 64          conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 128, 161, 32) 128         max_pooling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "1x1_conv__1 (Conv2D)            (None, 128, 161, 32) 1056        batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "3x3_conv__1 (Conv2D)            (None, 128, 161, 32) 18464       batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "5x5_conv__1 (Conv2D)            (None, 128, 161, 32) 12832       batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 128, 161, 32) 1056        batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 128, 161, 128 0           1x1_conv__1[0][0]                \n",
      "                                                                 3x3_conv__1[0][0]                \n",
      "                                                                 5x5_conv__1[0][0]                \n",
      "                                                                 conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "incept_2_input (Concatenate)    (None, 128, 161, 160 0           concatenate_3[0][0]              \n",
      "                                                                 incept_1_input[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 128, 161, 160 640         incept_2_input[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 128, 161, 160 640         incept_2_input[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 128, 161, 64) 10304       batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 128, 161, 16) 2576        batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2D)  (None, 128, 161, 160 0           incept_2_input[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 128, 161, 160 640         incept_2_input[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 128, 161, 64) 256         conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 128, 161, 16) 64          conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, 128, 161, 160 640         max_pooling2d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "1x1_conv__2 (Conv2D)            (None, 128, 161, 32) 5152        batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "3x3_conv__2 (Conv2D)            (None, 128, 161, 32) 18464       batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "5x5_conv__2 (Conv2D)            (None, 128, 161, 32) 12832       batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 128, 161, 32) 5152        batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 128, 161, 128 0           1x1_conv__2[0][0]                \n",
      "                                                                 3x3_conv__2[0][0]                \n",
      "                                                                 5x5_conv__2[0][0]                \n",
      "                                                                 conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "incept_3_input (Concatenate)    (None, 128, 161, 288 0           incept_2_input[0][0]             \n",
      "                                                                 concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNo (None, 128, 161, 288 1152        incept_3_input[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNo (None, 128, 161, 288 1152        incept_3_input[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 128, 161, 64) 18496       batch_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 128, 161, 16) 4624        batch_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2D)  (None, 128, 161, 288 0           incept_3_input[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, 128, 161, 288 1152        incept_3_input[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, 128, 161, 64) 256         conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_37 (BatchNo (None, 128, 161, 16) 64          conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_38 (BatchNo (None, 128, 161, 288 1152        max_pooling2d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "1x1_conv__3 (Conv2D)            (None, 128, 161, 32) 9248        batch_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "3x3_conv__3 (Conv2D)            (None, 128, 161, 32) 18464       batch_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "5x5_conv__3 (Conv2D)            (None, 128, 161, 32) 12832       batch_normalization_37[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 128, 161, 32) 9248        batch_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 128, 161, 128 0           1x1_conv__3[0][0]                \n",
      "                                                                 3x3_conv__3[0][0]                \n",
      "                                                                 5x5_conv__3[0][0]                \n",
      "                                                                 conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "incept_3_output (Concatenate)   (None, 128, 161, 416 0           concatenate_5[0][0]              \n",
      "                                                                 incept_3_input[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 128, 161, 32) 13344       incept_3_output[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_39 (BatchNo (None, 128, 161, 32) 128         conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2D)  (None, 64, 80, 32)   0           batch_normalization_39[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_1 (Glo (None, 32)           0           max_pooling2d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 8)            264         global_average_pooling2d_1[0][0] \n",
      "==================================================================================================\n",
      "Total params: 186,264\n",
      "Trainable params: 181,816\n",
      "Non-trainable params: 4,448\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# aws stuff\n",
    "# import s3fs\n",
    "# fs = s3fs.S3FileSystem() \n",
    "# with fs.open(\"spectrogramdatabucket/Spectrogram_Data_Labels.npy\") as f:\n",
    "#     labels = np.load(f)\n",
    "# with fs.open(\"spectrogramdatabucket/Spectrogram_Data.npy\") as d:\n",
    "#     data = np.load(d)\n",
    "    \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score\n",
    "from tensorflow.python.keras.models import Model\n",
    "from tensorflow.python.keras.layers import (Dense, Dropout, Flatten, Conv2D, MaxPooling2D,\n",
    "                                            BatchNormalization, Input, concatenate, GlobalAveragePooling2D)\n",
    "from tensorflow.python.keras import utils\n",
    "import json\n",
    "import tensorflow as tf\n",
    " \n",
    "def inception_module(layer_in, f1, f2_in, f2_out, f3_in, f3_out, f4_out, n):\n",
    "    # 1x1 conv\n",
    "    conv1 = Conv2D(f1, (1,1), padding='same', activation='relu',name=f'1x1_conv__{n}')(BatchNormalization(axis=-1)(layer_in))\n",
    "    # 3x3 conv\n",
    "    conv3 = Conv2D(f2_in, (1,1), padding='same', activation='relu',)(BatchNormalization(axis=-1)(layer_in))\n",
    "    conv3 = Conv2D(f2_out, (3,3), padding='same', activation='relu',name=f'3x3_conv__{n}')(BatchNormalization(axis=-1)(conv3))\n",
    "    # 5x5 conv\n",
    "    conv5 = Conv2D(f3_in, (1,1), padding='same', activation='relu')(BatchNormalization(axis=-1)(layer_in))\n",
    "    conv5 = Conv2D(f3_out, (5,5), padding='same', activation='relu',name=f'5x5_conv__{n}')(BatchNormalization(axis=-1)(conv5))\n",
    "    # 3x3 max pooling\n",
    "    pool = MaxPooling2D((3,3), strides=(1,1), padding='same')(layer_in)\n",
    "    pool = Conv2D(f4_out, (1,1), padding='same', activation='relu')(BatchNormalization(axis=-1)(pool))\n",
    "    # concatenate filters, assumes filters/channels last\n",
    "    layer_out = concatenate([conv1, conv3, conv5, pool], axis=-1)\n",
    "    return layer_out\n",
    " \n",
    "\n",
    "paddings = tf.constant([[0, 0],[1,1],[1,1],[0,0]])\n",
    "visible = Input(shape=(128,647,1))\n",
    "padded_input = tf.pad(visible,paddings,\"CONSTANT\")\n",
    "x = Conv2D(32,(3,3),activation='relu',input_shape=(128,647,1))(padded_input)\n",
    "x = BatchNormalization(axis=-1)(x)\n",
    "incept_1_input = MaxPooling2D(pool_size=(1,4),name='incept_1_input')(x)\n",
    " \n",
    "f1 = 32\n",
    "f2_in = 64\n",
    "f2_out = 32\n",
    "f3_in = 16\n",
    "f3_out = 32\n",
    "f4_out = 32\n",
    " \n",
    "#Inception Module\n",
    "incept_1 = inception_module(incept_1_input, f1, f2_in, f2_out, f3_in, f3_out, f4_out,1)\n",
    " \n",
    "incept_2_input = concatenate([incept_1,incept_1_input],name='incept_2_input')\n",
    "incept_2 = inception_module(incept_2_input, f1, f2_in, f2_out, f3_in, f3_out, f4_out,2)\n",
    " \n",
    "incept_3_input = concatenate([incept_2_input,incept_2],name='incept_3_input')\n",
    "incept_3 = inception_module(incept_3_input, f1, f2_in, f2_out, f3_in, f3_out, f4_out,3)\n",
    " \n",
    "incept_3_output = concatenate([incept_3,incept_3_input],name='incept_3_output')\n",
    " \n",
    "#Transition Layers\n",
    "x = BatchNormalization(axis=-1)(Conv2D(32,(1,1))(incept_3_output))\n",
    "x = MaxPooling2D(pool_size=(2,2),strides=2)(x)\n",
    " \n",
    "#Decision Layers\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "out = Dense(8, activation=\"softmax\")(x)\n",
    " \n",
    "model = Model(inputs=visible, outputs=out)\n",
    " \n",
    "model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    " \n",
    "model.summary()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Shape: (39169, 128, 647, 1) ... (39169, 8)\n",
      "Testing Shape: (10912, 128, 647, 1) ... (10912, 8)\n",
      "Validation Shape: (5876, 128, 647, 1) ... (5876, 8)\n"
     ]
    }
   ],
   "source": [
    "labels = np.load(\"../Data/FMA_Small_Spectrogram_Data_Labels.npy\")\n",
    "data = np.load(\"../Data/FMA_Small_Spectrogram_Data.npy\")\n",
    "\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.30, shuffle=True)\n",
    "X_train = X_train.reshape(X_train.shape[0], 128, 647, 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], 128, 647, 1)\n",
    "\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=0.35, shuffle=True)\n",
    "\n",
    "print(\"Training Shape: {} ... {}\".format(X_train.shape,y_train.shape))\n",
    "print(\"Testing Shape: {} ... {}\".format(X_test.shape,y_test.shape))\n",
    "print(\"Validation Shape: {} ... {}\".format(X_val.shape,y_val.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ind = 39169 - 2000\n",
    "test_ind = 13430 + 500\n",
    "val_ind = 3358 + 1500\n",
    " \n",
    "X_train = data[0:train_ind,:,:]\n",
    "y_train = labels[0:train_ind,:]\n",
    " \n",
    "X_test = data[0:test_ind,:,:]\n",
    "y_test = labels[0:test_ind,:]\n",
    " \n",
    "X_val = data[0:val_ind,:,:]\n",
    "y_val = labels[0:val_ind,:]\n",
    " \n",
    "X_train = X_train.reshape(X_train.shape[0], 128, 647, 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], 128, 647, 1)\n",
    "X_val = X_val.reshape(X_val.shape[0], 128, 647, 1)\n",
    " \n",
    "print(\"Training Shape: {} ... {}\".format(X_train.shape,y_train.shape))\n",
    "print(\"Testing Shape: {} ... {}\".format(X_test.shape,y_test.shape))\n",
    "print(\"Validation Shape: {} ... {}\".format(X_val.shape,y_val.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train,\n",
    "                        y_train, \n",
    "                        batch_size=32,\n",
    "                        validation_data=(X_test, y_test),\n",
    "                        epochs=30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D, BatchNormalization\n",
    "from tensorflow.python.keras import utils\n",
    "import json\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.utils import plot_model\n",
    "import tifffile \n",
    "\n",
    "with open(\"../Genre_Track_Id_Dict.json\",'r') as j:\n",
    "        id_genre_dict = json.load(j)\n",
    "numerical_labels = dict(zip(list(id_genre_dict.keys()),np.arange(0,8)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Shape: (39169, 128, 647, 1) ... (39169, 8)\n",
      "Testing Shape: (13430, 128, 647, 1) ... (13430, 8)\n",
      "Validation Shape: (3358, 128, 647, 1) ... (3358, 8)\n"
     ]
    }
   ],
   "source": [
    "data = np.zeros((56000,128,647))\n",
    "labels = np.zeros((56000,len(numerical_labels)))\n",
    "\n",
    "ct=-1\n",
    "for genre in os.listdir(\"../Data/mp3_files_fma_small\"):\n",
    "    genre_dir = os.path.join(\"../Data/mp3_files_fma_small\",genre)\n",
    "    for fname in [f for f in os.listdir(genre_dir) if \".tiff\" in f]:\n",
    "        mel_db_path = os.path.join(genre_dir,fname)\n",
    "\n",
    "        spect = tifffile.imread(mel_db_path)\n",
    "        if spect.shape[1] == 646:\n",
    "            spect = np.hstack((spect,np.zeros((128,1))))\n",
    "        if spect.shape[1] == 647:\n",
    "            ct+=1\n",
    "            data[ct,:,:] = spect\n",
    "            genre_encode = numerical_labels[genre] \n",
    "            labels[ct][genre_encode] = 1\n",
    "data = data[0:ct,:,:]\n",
    "labels = labels[0:ct,:]\n",
    "\n",
    "data = data/-80\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.30, shuffle=True)\n",
    "X_train = X_train.reshape(X_train.shape[0], 128, 647, 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], 128, 647, 1)\n",
    "\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=0.35, shuffle=True)\n",
    "\n",
    "print(\"Training Shape: {} ... {}\".format(X_train.shape,y_train.shape))\n",
    "print(\"Testing Shape: {} ... {}\".format(X_test.shape,y_test.shape))\n",
    "print(\"Validation Shape: {} ... {}\".format(X_val.shape,y_val.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"../Data/fma_small_npy_format/Testing_Data\":X_test,\n",
    "         \"../Data/fma_small_npy_format/Testing_Labels\":y_test,\n",
    "         \"../Data/fma_small_npy_format/Training_Data\":X_train,\n",
    "         \"../Data/fma_small_npy_format/Training_Labels\":y_train,\n",
    "         \"../Data/fma_small_npy_format/Validation_Data\":X_val,\n",
    "         \"../Data/fma_small_npy_format/Validation_Labels\":y_val}\n",
    "\n",
    "ct = 0\n",
    "for outdir,data_tensor in config.items():\n",
    "    \n",
    "    \n",
    "    for arr in data_tensor:\n",
    "        ct +=1\n",
    "        fname = outdir.split(\"/\")[-1] +\"_{}.npy\".format(ct)\n",
    "        outpath = os.path.join(outdir,fname)\n",
    "        np.save(outpath,arr)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for creating a projected inception module\n",
    "def inception_module(layer_in, f1, f2_in, f2_out, f3_in, f3_out, f4_out, n):\n",
    "    # 1x1 conv\n",
    "    conv1 = Conv2D(f1, (1,1), padding='same', activation='relu',name=f'1x1_conv__{n}')(BatchNormalization(axis=-1)(layer_in))\n",
    "    # 3x3 conv\n",
    "    conv3 = Conv2D(f2_in, (1,1), padding='same', activation='relu',)(BatchNormalization(axis=-1)(layer_in))\n",
    "    conv3 = Conv2D(f2_out, (3,3), padding='same', activation='relu',name=f'3x3_conv__{n}')(BatchNormalization(axis=-1)(conv3))\n",
    "    # 5x5 conv\n",
    "    conv5 = Conv2D(f3_in, (1,1), padding='same', activation='relu')(BatchNormalization(axis=-1)(layer_in))\n",
    "    conv5 = Conv2D(f3_out, (5,5), padding='same', activation='relu',name=f'5x5_conv__{n}')(BatchNormalization(axis=-1)(conv5))\n",
    "    # 3x3 max pooling\n",
    "    pool = MaxPooling2D((3,3), strides=(1,1), padding='same')(layer_in)\n",
    "    pool = Conv2D(f4_out, (1,1), padding='same', activation='relu')(BatchNormalization(axis=-1)(pool))\n",
    "    # concatenate filters, assumes filters/channels last\n",
    "    layer_out = concatenate([conv1, conv3, conv5, pool], axis=-1)\n",
    "    return layer_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "paddings = tf.constant([[0, 0],[1,1],[1,1],[0,0]])\n",
    "visible = Input(shape=(128,647,1))\n",
    "\n",
    "padded_input = tf.pad(visible,paddings,\"CONSTANT\")\n",
    "x = Conv2D(32,(3,3),activation='relu',input_shape=(128,647,1))(padded_input)\n",
    "x = BatchNormalization(axis=-1)(x)\n",
    "incept_1_input = layers.MaxPooling2D(pool_size=(1,4),name='incept_1_input')(x)\n",
    "\n",
    "f1 = 32\n",
    "f2_in = 64\n",
    "f2_out = 32\n",
    "f3_in = 16\n",
    "f3_out = 32\n",
    "f4_out = 32\n",
    "\n",
    "#Inception Module\n",
    "incept_1 = inception_module(incept_1_input, f1, f2_in, f2_out, f3_in, f3_out, f4_out,1)\n",
    "\n",
    "incept_2_input = concatenate([incept_1,incept_1_input],name='incept_2_input')\n",
    "incept_2 = inception_module(incept_2_input, f1, f2_in, f2_out, f3_in, f3_out, f4_out,2)\n",
    "\n",
    "incept_3_input = concatenate([incept_2_input,incept_2],name='incept_3_input')\n",
    "incept_3 = inception_module(incept_3_input, f1, f2_in, f2_out, f3_in, f3_out, f4_out,3)\n",
    "\n",
    "incept_3_output = concatenate([incept_3,incept_3_input],name='incept_3_output')\n",
    "\n",
    "#Transition Layers\n",
    "x = BatchNormalization(axis=-1)(Conv2D(32,(1,1))(incept_3_output))\n",
    "x = layers.MaxPooling2D(pool_size=(2,2),strides=2)(x)\n",
    "\n",
    "#Decision Layers\n",
    "x = layers.GlobalAveragePooling2D()(x)\n",
    "out = layers.Dense(8, activation=\"softmax\")(x)\n",
    "\n",
    "model = Model(inputs=visible, outputs=out)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train,\n",
    "                        y_train, \n",
    "                        batch_size=32,\n",
    "                        validation_data=(X_test, y_test),\n",
    "                        epochs=30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
